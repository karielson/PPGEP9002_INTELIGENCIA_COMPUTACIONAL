{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2v6WNBjvgPTn+Cg/BgQzh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karielson/PPGEP9002_INTELIGENCIA_COMPUTACIONAL/blob/main/Classifica%C3%A7%C3%A3o_de_Textos_usando_Redes_Neurais_e_M%C3%A9todos_de_Classifica%C3%A7%C3%A3o_Supervisionada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de Textos usando Redes Neurais e Métodos Supervisionados\n",
        "\n",
        "**Aluno:** Karielson Medeiros Feitosa  \n",
        "**Disciplina:**  PPGEP9002 - 2024.2  \n",
        "**Professor:** José Alfredo F. Costa  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introdução\n",
        "Esta tarefa tem como objetivo aplicar diferentes métodos de classificação de textos utilizando redes neurais e técnicas de aprendizado supervisionado. A base de dados utilizada contém textos categorizados em 6 classes diferentes, abrangendo diversas áreas temáticas.\n",
        "\n",
        "### Objetivo\n",
        "Explorar diferentes representações textuais e arquiteturas de redes neurais para comparar o desempenho dos modelos no problema de classificação de textos.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_6PWsUE13xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Pré-Processamento de Textos\n",
        "\n",
        "Nesta seção, serão realizadas as seguintes etapas de pré-processamento:\n",
        "1. Conversão para minúsculas.\n",
        "2. Remoção de pontuações e caracteres especiais.\n",
        "3. Remoção de stopwords.\n",
        "4. Tokenização e lematização.\n",
        "5. Divisão dos dados em treino, validação e teste (70%-15%-15%).\n"
      ],
      "metadata": {
        "id": "pmv6ei2D601v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raGIjcOo1uaG",
        "outputId": "12c7b650-e6f8-4c0a-e50b-7718503872d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Informações da base de dados:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 319 entries, 0 to 318\n",
            "Data columns (total 4 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Texto Original   315 non-null    object\n",
            " 1   Texto Expandido  315 non-null    object\n",
            " 2   Classe           319 non-null    int64 \n",
            " 3   Categoria        319 non-null    object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 10.1+ KB\n",
            "None\n",
            "                                      Texto Original  \\\n",
            "0  Desenvolvimento de criptomoedas e blockchain: ...   \n",
            "1  Economia colaborativa: plataformas que revoluc...   \n",
            "2  Economia criativa no audiovisual: potencial de...   \n",
            "3  Economia do conhecimento: investimentos em edu...   \n",
            "4  Indústria 4.0 no Brasil: transformação digital...   \n",
            "\n",
            "                                     Texto Expandido  Classe Categoria  \n",
            "0  O Brasil emerge como um polo de inovação no me...       0  Economia  \n",
            "1  Plataformas de economia colaborativa estão red...       0  Economia  \n",
            "2  O setor audiovisual brasileiro apresenta cresc...       0  Economia  \n",
            "3  A economia do conhecimento se torna estratégic...       0  Economia  \n",
            "4  A Indústria 4.0 representa uma revolução tecno...       0  Economia  \n",
            "Removendo linhas com valores nulos...\n"
          ]
        }
      ],
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import torch\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Baixar recursos do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Adicionado para resolver dependência de lematização\n",
        "\n",
        "# Carregar a base de dados (substituir 'file_path' pelo caminho do arquivo no Google Colab)\n",
        "file_path = '/content/Base_dados_textos_6_classes.csv'\n",
        "df = pd.read_csv(file_path, sep=';', encoding='latin1')  # Corrigir encoding para 'latin1' para lidar com caracteres especiais\n",
        "\n",
        "# Exibir informações da base de dados\n",
        "print(\"Informações da base de dados:\")\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "\n",
        "# Remover linhas com valores nulos\n",
        "print(\"Removendo linhas com valores nulos...\")\n",
        "df.dropna(subset=['Texto Original', 'Classe'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Função para limpeza do texto\n",
        "def preprocess_text(text):\n",
        "    if pd.isnull(text):  # Verificar se o texto é nulo\n",
        "        return ''\n",
        "    text = text.lower()  # Converter para minúsculas\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remover pontuações e caracteres especiais\n",
        "    tokens = word_tokenize(text)  # Tokenizar\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]  # Lematizar e remover stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Aplicar o pré-processamento\n",
        "try:\n",
        "    df['Texto Limpo'] = df['Texto Original'].apply(preprocess_text)\n",
        "except LookupError as e:\n",
        "    print(f\"Erro durante o pré-processamento: {e}\")\n",
        "    nltk.download('punkt_tab')  # Adicionando tentativa de baixar recurso adicional\n",
        "    df['Texto Limpo'] = df['Texto Original'].apply(preprocess_text)\n",
        "\n",
        "# Verificar se todas as colunas necessárias estão presentes\n",
        "if 'Texto Limpo' in df.columns and 'Classe' in df.columns:\n",
        "    # Dividir a base de dados em treino, validação e teste (70%-15%-15%)\n",
        "    train_data, temp_data, train_labels, temp_labels = train_test_split(df['Texto Limpo'], df['Classe'], test_size=0.30, random_state=42)\n",
        "    val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.50, random_state=42)\n",
        "else:\n",
        "    raise KeyError(\"As colunas 'Texto Limpo' e 'Classe' são necessárias para a divisão dos dados e não foram encontradas.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Geração de Representações Textuais\n",
        "\n",
        "Nesta etapa, utilizaremos três técnicas de geração de embeddings:\n",
        "1. **TF-IDF**\n",
        "2. **Word2Vec**\n",
        "3. **Transformers (BERT)**\n",
        "\n",
        "### 3.1 TF-IDF"
      ],
      "metadata": {
        "id": "iWpHfZlI8Wpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Criar embeddings usando TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(train_data)\n",
        "X_val_tfidf = vectorizer.transform(val_data)\n",
        "X_test_tfidf = vectorizer.transform(test_data)"
      ],
      "metadata": {
        "id": "EYGYgZlR8Vl6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6S4rIz_w8cps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.2 Word2Vec"
      ],
      "metadata": {
        "id": "neaZcAcK8eO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenizar os textos para Word2Vec\n",
        "tokenized_train = [text.split() for text in train_data]\n",
        "\n",
        "# Treinar o modelo Word2Vec\n",
        "word2vec_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Função para gerar embeddings a partir do Word2Vec\n",
        "def get_word2vec_embeddings(data, model):\n",
        "    embeddings = []\n",
        "    for text in data:\n",
        "        words = text.split()\n",
        "        word_embeddings = [model.wv[word] for word in words if word in model.wv]\n",
        "        if word_embeddings:\n",
        "            embeddings.append(sum(word_embeddings) / len(word_embeddings))\n",
        "        else:\n",
        "            embeddings.append([0] * model.vector_size)\n",
        "    return embeddings\n",
        "\n",
        "X_train_w2v = get_word2vec_embeddings(train_data, word2vec_model)\n",
        "X_val_w2v = get_word2vec_embeddings(val_data, word2vec_model)\n",
        "X_test_w2v = get_word2vec_embeddings(test_data, word2vec_model)"
      ],
      "metadata": {
        "id": "nHDDKzqu8e2T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Transformers (BERT)"
      ],
      "metadata": {
        "id": "9VIQlpD38mQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Carregar modelo e tokenizer BERT\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Função para gerar embeddings usando BERT\n",
        "def get_bert_embeddings(data):\n",
        "    embeddings = []\n",
        "    for text in data:\n",
        "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        outputs = bert_model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
        "    return embeddings\n",
        "\n",
        "X_train_bert = get_bert_embeddings(train_data)\n",
        "X_val_bert = get_bert_embeddings(val_data)\n",
        "X_test_bert = get_bert_embeddings(test_data)"
      ],
      "metadata": {
        "id": "EgLf7f1U8l66"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Modelos de Classificação\n",
        "\n",
        "Nesta seção, implementaremos três modelos de classificação:\n",
        "1. **MLP (Multilayer Perceptron)**\n",
        "2. **CNN (Redes Convolucionais)**\n",
        "3. **Transformers (Fine-Tuning de BERT)**\n",
        "\n",
        "### 4.1 MLP"
      ],
      "metadata": {
        "id": "ymbaf9KF8taa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Treinar o modelo MLP\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
        "mlp.fit(X_train_tfidf, train_labels)\n",
        "\n",
        "# Avaliar o modelo\n",
        "predictions = mlp.predict(X_test_tfidf)\n",
        "print(\"Acurácia:\", accuracy_score(test_labels, predictions))\n",
        "print(\"Relatório de Classificação:\\n\", classification_report(test_labels, predictions))\n",
        "print(\"Matriz de confusão:\\n\", confusion_matrix(test_labels, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0FjYEZN8ucQ",
        "outputId": "391584b2-4da0-4208-f2e9-3015debbd01c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.75\n",
            "Relatório de Classificação:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80         8\n",
            "           1       0.70      0.88      0.78         8\n",
            "           2       0.75      0.43      0.55         7\n",
            "           3       0.62      0.62      0.62         8\n",
            "           4       0.75      1.00      0.86         9\n",
            "           5       0.86      0.75      0.80         8\n",
            "\n",
            "    accuracy                           0.75        48\n",
            "   macro avg       0.76      0.74      0.73        48\n",
            "weighted avg       0.76      0.75      0.74        48\n",
            "\n",
            "Matriz de confusão:\n",
            " [[6 1 0 0 1 0]\n",
            " [1 7 0 0 0 0]\n",
            " [0 0 3 3 0 1]\n",
            " [0 1 1 5 1 0]\n",
            " [0 0 0 0 9 0]\n",
            " [0 1 0 0 1 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 CNN"
      ],
      "metadata": {
        "id": "oL1zBRWK-fjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(input_dim=5000, output_dim=100, input_length=X_train_tfidf.shape[1]))\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "cnn_model.add(GlobalMaxPooling1D())\n",
        "cnn_model.add(Dense(10, activation='relu'))\n",
        "cnn_model.add(Dropout(0.5))\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(np.array(X_train_tfidf.todense()), np.array(train_labels), epochs=5, batch_size=32, validation_data=(np.array(X_val_tfidf.todense()), np.array(val_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvedQbtL-e9M",
        "outputId": "e654a2c3-7b4f-4c61-968e-f11a534b38a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 745ms/step - accuracy: 0.1402 - loss: -0.3519 - val_accuracy: 0.1875 - val_loss: -4.8170\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 340ms/step - accuracy: 0.2000 - loss: -10.9282 - val_accuracy: 0.1875 - val_loss: -21.3331\n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 381ms/step - accuracy: 0.1729 - loss: -43.0261 - val_accuracy: 0.1875 - val_loss: -61.2461\n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - accuracy: 0.1718 - loss: -112.9620 - val_accuracy: 0.1875 - val_loss: -138.8782\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 315ms/step - accuracy: 0.1712 - loss: -224.0119 - val_accuracy: 0.1875 - val_loss: -272.0915\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a9d80677340>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Avaliar CNN"
      ],
      "metadata": {
        "id": "6sSiBC4j-rOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_predictions = cnn_model.predict(np.array(X_test_tfidf.todense()))\n",
        "cnn_predictions = (cnn_predictions > 0.5).astype(int)\n",
        "print(\"Acurácia CNN:\", accuracy_score(test_labels, cnn_predictions))\n",
        "print(\"Relatório de Classificação CNN:\\n\", classification_report(test_labels, cnn_predictions))\n",
        "print(\"Matriz de confusão CNN:\\n\", confusion_matrix(test_labels, cnn_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbS27weN-twv",
        "outputId": "33e068aa-92c7-468c-c2d3-9806ee515c3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
            "Acurácia CNN: 0.16666666666666666\n",
            "Relatório de Classificação CNN:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         8\n",
            "           1       0.17      1.00      0.29         8\n",
            "           2       0.00      0.00      0.00         7\n",
            "           3       0.00      0.00      0.00         8\n",
            "           4       0.00      0.00      0.00         9\n",
            "           5       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.17        48\n",
            "   macro avg       0.03      0.17      0.05        48\n",
            "weighted avg       0.03      0.17      0.05        48\n",
            "\n",
            "Matriz de confusão CNN:\n",
            " [[0 8 0 0 0 0]\n",
            " [0 8 0 0 0 0]\n",
            " [0 7 0 0 0 0]\n",
            " [0 8 0 0 0 0]\n",
            " [0 9 0 0 0 0]\n",
            " [0 8 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1FxQXYIi-ybQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Fine-Tuning de BERT"
      ],
      "metadata": {
        "id": "oHNWtddr-zFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_fine_tune_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
        "optimizer = AdamW(bert_fine_tune_model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEQu7mX1-3RZ",
        "outputId": "299ca40a-2293-47ea-ca04-f77d2907a499"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar os dados para DataLoader\n",
        "def prepare_dataloader(data, labels, tokenizer, max_length=512, batch_size=16):\n",
        "    inputs = tokenizer(list(data), return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n",
        "    return DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "train_loader = prepare_dataloader(train_data, train_labels, tokenizer)\n",
        "val_loader = prepare_dataloader(val_data, val_labels, tokenizer)\n",
        "test_loader = prepare_dataloader(test_data, test_labels, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "oYfrVjAG-6Eq",
        "outputId": "8abec1e8-71d6-4cb4-ab4c-30f19f12f394"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-226ab8b75bdb>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-226ab8b75bdb>\u001b[0m in \u001b[0;36mprepare_dataloader\u001b[0;34m(data, labels, tokenizer, max_length, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3"
          ]
        }
      ]
    }
  ]
}